記述1｜線形回帰推定器は、すべての不偏推定器の中でも最も分散が少ない。記述2｜AddBoostによって組み立てられた分類器に割り当てられた係数αは常に非負である。,真、真,偽、偽,真、偽,偽、真,D
記述1｜RoBERTaは、BERTが事前学習したコーパスの約10倍の大きさのコーパスで事前学習する。記述2｜2018年のResNeXtsは、通常、tanh活性化関数を使用していた。,真、真,偽、偽,真、偽,偽、真,C
記述1｜サポートベクターマシンは、ロジスティック回帰モデルのように、入力例が与えられたときに可能なラベルの確率分布を与える。記述2｜線形カーネルから高次の多項式カーネルに移行しても、一般的にサポートベクターは変わらないと予想される。,真、真,偽、偽,真、偽,偽、真,B
機械学習問題には、4つの属性と1つのクラスが含まれる。属性にはそれぞれ3、2、2、2の可能な値がある。また、クラスには3つの可能な値がある。最大いくつの異なる例があり得るか。,12,24,48,72,D
2020年現在、高解像度画像の分類に最適なアーキテクチャはどれか。,畳み込みネットワーク,グラフネットワーク,全結合ネットワーク,RBFネットワーク,A
記述1｜データの対数尤度は、EMアルゴリズムの連続した反復によって常に増加する。記述2｜Q学習の欠点のひとつは、学習者の行動が環境にどのような影響を与えるかについて予備知識がある場合にしか使えないことである。,真、真,偽、偽,真、偽,偽、真,B
コスト関数の勾配を計算し、それをベクトルgに格納したとする。勾配が与えられた場合、1回の勾配降下の更新のコストは次のうちどれか。,O(D),O(N),O(ND),O(ND^2),A
記述1｜連続型確率変数xとその確率分布関数p(x)について、すべてのxに対して0≤p(x)≤1が成り立つ。記述2｜決定木は情報利得を最小化することによって学習される。,真、真,偽、偽,真、偽,偽、真,B
以下のベイジアンネットワークを考察する。ベイジアンネットワークH -> U <- P <- Wに必要な独立パラメータはいくつあるか。,2,4,8,16,C
学習例の数が無限大になると、そのデータで学習したモデルはどのようになるか。,分散が小さくなる,分散が大きくなる,分散は同じ,上記のいずれでもない,A
記述1｜2次元平面上のすべての長方形の集合（軸平行ではない長方形を含む）は、5点の集合を砕くことができる。記述2｜k = 1のときのk最近傍分類器のVC次元は無限大である。,真、真,偽、偽,真、偽,偽、真,A
__ は、学習データをモデル化することも、新しいデータに汎化することもできないモデルを指す。,良適合,過剰適合,過少適合,上記のすべて,C
記述1｜F1スコアはクラスの不均衡が大きいデータセットに特に有用である。記述2｜ROC曲線下の面積は、異常検知機能を評価するため主な指標の1つである。,真、真,偽、偽,真、偽,偽、真,A
記述1｜バックプロパゲーションアルゴリズムは、隠れ層を持つ大域的に最適なニューラルネットワークを学習する。記述2｜どの線でも砕けない3点のケースが少なくとも1つ見つかるため、線のVC次元は最大でも2であるべきである。,真、真,偽、偽,真、偽,偽、真,B
高エントロピーとは、分類の区切りが以下のどれであることを意味するか。,純粋,非純粋,有用,無用,B
記述1｜元のResNet論文ではバッチ正規化ではなくレイヤー正規化が使われている。記述2｜DCGANは学習を安定化させるために自己注意機構を使う。,真、真,偽、偽,真、偽,偽、真,B
あるデータセットの線形回帰モデルを構築する際、ある特徴の係数が比較的高い負の値であることを観察した。これは次のどのことを示唆しているか。,この特徴はモデルに強い影響を与える（保持すべき）,この特徴はモデルに強い影響を与えない（無視すべき）,追加の情報がなければこの特徴の重要性についてコメントできない,何も決定できない,C
ニューラルネットワークにおいて、過少適合（すなわち高バイアスモデル）と過剰適合（すなわち高分散モデル）のトレードオフに最も影響する構造仮定は次のうちどれか。,隠れノードの数,学習率,重みの初期選択,定項単位の入力の使用,A
多項式回帰において、過少適合と過剰適合のトレードオフに最も影響する構造的仮定はどれか。,多項式の次数,重みを行列反転で学習するか、勾配降下で学習するか,ガウス雑音の推定分散,定項単位入力の使用,A
記述1｜2020年現在、CIFAR-10で98%以上の精度のモデルもある。記述2｜元のResNetはAdamオプティマイザで最適化されていない。,真、真,偽、偽,真、偽,偽、真,A
K平均アルゴリズムでは...,特徴空間の次元がサンプルの数より大きくないことが必要,K = 1のとき、目的関数の値が最小になる,所定のクラスタ数のクラス内分散を最小化する,初期平均がサンプル自身の一部として選ばれる場合に限り、大域最適解に収束する,C
記述1｜VGGNetはAlexNetの第1層カーネルよりも幅と高さが小さい畳み込みカーネルを持つ。記述2｜データ依存の重み初期化手法はバッチ正規化より前に導入された。,真、真,偽、偽,真、偽,偽、真,A
"行列A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]の階数を答えよ。",0,1,2,3,B
記述1｜密度推定（たとえばカーネル密度推定器を使用）は、分類するために使用できる。記述2｜ロジスティック回帰と（同一のクラス共分散を持つ）ガウスナイーブベイズ間の対応は、2つの分類器のパラメータの間に1対1の対応があることを意味する。,真、真,偽、偽,真、偽,偽、真,C
住宅の幾何学的位置のような空間データに対してクラスタリングを行うとする。サイズと形状がさまざまに異なるクラスタを作成する場合、最適な方法はどれか。,決定木,密度ベースのクラスタリング,モデルベースのクラスタリング,K平均法,B
記述1｜AdaBoostでは、誤分類された例の重みは、同じ乗法因子だけ増加する。記述2｜AdaBoostでは、重みがD_tの訓練データのt番目に弱い分類器の重み付きトレーニングエラーe_tは、tの関数として増加する傾向がある。,真、真,偽、偽,真、偽,偽、真,A
MLE推定値は、次のどの理由から望ましくないことが多いか。,バイアスがかかっているから,分散が大きいから,一貫性のない推定値であるから,上記のいずれでもない,B
勾配降下法の計算複雑度は...,Dに線形,Nに線形,Dの多項式,反復回数に依存,C
複数の決定木の出力を平均化するとどうなるか。,バイアスが増加する,バイアスが減少する,分散が増加する,分散が減少する,D
識別された特徴の部分集合に線形回帰を適用して得られるモデルは、次のどの選択において、部分集合を識別するプロセスの最後に得られるモデルと異なる可能性があるか。,ベストサブセット選択,前方ステップワイズ選択,前方ステージワイズ選択,上記のすべて,C
次のうち、ニューラルネットワークについて正しい記述はどれか。,凸目的関数を最適化する,確率勾配降下法でのみ訓練可能,異なる活性化関数を組み合わせて使用可能,上記のいずれでもない,C
疾病Dの発生率が100人あたり約5例であるとする（すなわち、P(D)=0.05）。ブール確率変数Dは患者が「疾病Dに罹患している」ことを意味し、ブール確率変数TPは「検査結果が陽性」を表すものとする。疾病Dの検査は非常に正確であることが知られており、罹患していると陽性となる確率が0.99で、罹患していなければ陰性となる確率が0.97である。陽性となる事前確率であるP(TP)は何か。,0.0368,0.473,0.078,上記のいずれでもない,C
記述1｜放射基底カーネル関数を通して特徴空間Qにマッピングされた後、非加重ユークリッド距離を使用する1-NNは、元の空間よりも良好な分類性能を達成できる可能性がある（ただしこれは保証されない）。記述2｜パーセプトロンのVC次元は単純な線形SVMのVC次元より小さい。,真、真,偽、偽,真、偽,偽、真,B
グリッドサーチの欠点は次のうちどれか。,微分不可能関数には適用できない,非連続関数には適用できない,実装が難しい,複数の線形重回帰の実行速度が遅い,D
さまざまな手がかりに基づいてある地域の降雨量を予測することは、______の問題である。,教師あり学習,教師なし学習,クラスタリング,上記のいずれでもない,A
回帰について誤っている記述は次のうちどれか。,入力と出力を関連付ける,予測に使われる,解釈に使われることがある,因果関係を発見する,D
決定木を枝刈する主な理由はどれか。,テスト時の計算時間を節約するため,決定木を保存するスペースを節約するため,訓練セットの誤差を小さくするため,訓練セットの過剰適合を回避するため,D
記述1｜カーネル密度推定器は、元のデータ集合の各点Xiで値Yi = 1/nでカーネル回帰を実行することと等価である。記述2｜学習済みの決定木の深さは、木を作成するために使用された訓練例の数よりも大きくなることがある。,真、真,偽、偽,真、偽,偽、真,B
モデルが過剰適合しているとする。過剰適合を減らすために有効でない方法は次のうちどれか。,訓練データの量を増やす,誤差の最小化に使われる最適化アルゴリズムを改善する,モデルの複雑さを減らす,訓練データのノイズを減らす,B
記述1｜ソフトマックス関数はマルチクラスロジスティック回帰でよく使われる。記述2｜一様でないソフトマックス分布の温度は、そのエントロピーに影響する。,真、真,偽、偽,真、偽,偽、真,A
次のうち、SVMについて正しい記述はどれか。,2次元のデータポイントの場合、線形SVMが学習する分離超平面は直線になる,理論的には、ガウスカーネルSVMはどのような複雑な分離超平面もモデル化できない,SVMで使われるすべてのカーネル関数について、等価な閉形式の基底展開を得ることができる,SVMの過剰適合は、サポートベクターの数の関数ではない,A
所定のベイジアンネットワークH -> U <- P <- Wによって記述されるH、U、P、Wの同時確率は次のどれか。[注：条件付き確率の積として],"P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)",上記のいずれでもない,C
記述1｜放射基底カーネルを持つSVMのVC次元は無限大なので、このようなSVMはVC次元が有限の多項式カーネルを持つSVMよりも悪くなる。記述2｜線形活性化関数を持つ2層ニューラルネットワークは、本質的には、所定のデータセットで訓練された線形分離子の重み付き組み合わせである。線形分離器で構築されたブースティングアルゴリズムも線形分離器の組み合わせを見つけるので、これら2つのアルゴリズムは同じ結果を与える。,真、真,偽、偽,真、偽,偽、真,B
記述1｜ID3アルゴリズムは確実に最適な決定木を見つける。記述2｜どこでも0でない密度f()を持つ連続確率分布を考察する。ある値xの確率はf(x)に等しい。,真、真,偽、偽,真、偽,偽、真,B
N個の入力ノード、隠れ層なし、1つの出力ノード、エントロピー損失とシグモイド活性化関数を持つニューラルネットが与えられたとき、（適切なハイパーパラメータと初期化により）大域的最適解を見つけるためのアルゴリズムは次のうちどれか。,確率的勾配降下法,ミニバッチ勾配降下法,バッチ勾配降下法,上記のすべて,D
線形モデルで基底関数を増やす場合、最適な選択肢はどれか。,モデルバイアスを減少させる,推定バイアスを減少させる,分散を減少させる,バイアスと分散に影響を与えない,A
以下のベイジアンネットワークを考察する。独立性や条件付き独立性について仮定しない場合、独立パラメータはいくつ必要か。H -> U <- P <- W,3,4,7,15,D
分布外検出の別の用語は何か。,異常検出,1クラス検出,訓練時とテスト時の不一致ロバスト性,バックグラウンド検出,A
記述1｜弱い学習器hをブーストすることで分類器fを学習する。fの決定境界の関数形はhと同じだが、パラメータは異なる（たとえば，hが線形分類器であれば、fも線形分類器である）。記述2｜交差検証は、ブースティングの反復回数を選択するために使用できる。この手順は過剰適合を減らすのに役立つことがある。,真、真,偽、偽,真、偽,偽、真,D
記述1｜Highway networkはResNetの後に導入されたもので、畳み込みを優先して最大プーリングを避けている。記述2｜DenseNetは、通常、ResNetよりもメモリコストが高い。,真、真,偽、偽,真、偽,偽、真,D
訓練データセット内のインスタンス数をNとすると、最近傍の分類の実行時間は次のうちどれか。,O(1),O( N ),O(log N ),O( N^2 ),B
記述1｜元のResNetとTransformerはフィードフォワードニューラルネットワークである。記述2｜元のTransformerは自己注意機構を使うが、元のResNetはそうではない。,真、真,偽、偽,真、偽,偽、真,A
記述1｜RELUは単調ではないが、シグモイドは単調である。記述2｜勾配降下法で訓練されたニューラルネットワークは、高い確率で大域最適値に収束する。,真、真,偽、偽,真、偽,偽、真,D
ニューラルネットワークのシグモイドノードの数値出力の特徴は次のうちどれか。,無限であり、すべての実数を含む,無限であり、すべての整数を含む,0と1の間で制限がある,-1と1の間で制限がある,C
訓練データが線形分離可能な場合にのみ使用できるのは次のうちどれか。,線形ハードマージンSVM,線形ロジスティック回帰,線形ソフトマージンSVM,セントロイド法,A
空間クラスタリングアルゴリズムは次のうちどれか。,パーティショニングに基づくクラスタリング,K平均法,グリッドベースのクラスタリング,上記のすべて,D
記述1｜サポートベクターマシンが構築する最大マージンの決定境界は、すべての線形分類器の中で最も汎化誤差が小さい。記述2｜クラス条件付きのガウス分布を持つ生成モデルから得られるあらゆる決定境界は、原理的にはSVMと次数3以下の多項式カーネルで再現できる。,真、真,偽、偽,真、偽,偽、真,D
記述1｜線形モデルのL2正則化は、L1正則化よりもモデルをよりスパースにする傾向がある。記述2｜ResNetとTransformerには残差接続が見られる。,真、真,偽、偽,真、偽,偽、真,D
"P(H|E,F)を計算するが、条件付き独立性の情報がないとする。この計算に十分な数の集合は次のうちどれか。","P(E, F), P(H), P(E|H), P(F|H)","P(E, F), P(H), P(E, F|H)","P(H), P(E|H), P(F|H)","P(E, F), P(E|H), P(F|H)",B
バギングを行う際に、過剰適合を防ぐものは次のうちどれか。,サンプリング手法として置換サンプリングを使う,弱い分類器を使う,過剰適合を起こしにくい分類アルゴリズムを使う,訓練した分類器ごとに検証を行う,B
記述1｜PCAとスペクトルクラスタリング（Andrew Ng氏によるものなど）は、2つの異なる行列に対して固有値分解を行う。しかし、これら2つの行列のサイズは同じである。記述2｜分類は回帰の特別なケースなので、ロジスティック回帰は線形回帰の特別なケースである。,真、真,偽、偽,真、偽,偽、真,B
記述1｜Stanford Sentiment Treebankには、書評ではなく映画のレビューが含まれていた。記述2｜Penn Treebankは言語モデリングに使われている。,真、真,偽、偽,真、偽,偽、真,A
"行列A = [[3, 2, -9], [-6, -4, 18], [12, 8, -36]]のnull空間の次元数は、次のうちどれか。",0,1,2,3,C
サポートベクターとは何か。,決定境界から最も遠い例,SVMでf(x)を計算するために必要な唯一の例,データ重心,SVMにおいて重みαk が0でないすべての例,B
記述1｜Word2Vecのパラメータは制限ボルツマンマシンを使用して初期化されていない。記述2｜tanh関数は非線形活性化関数である。,真、真,偽、偽,真、偽,偽、真,A
訓練の損失がエポック数とともに増加する場合、学習過程に問題がある可能性があるのは次のうちどれか。,正則化が低すぎてモデルが過剰適合している,正則化が高すぎてモデルが過少適合している,ステップサイズが大きすぎる,ステップサイズが小さすぎる,C
疾病Dの発生率が100人あたり約5例であるとする（すなわち、P(D)=0.05）。ブール確率変数Dは患者が「疾病Dに罹患している」ことを意味し、ブール確率変数TPは「検査結果が陽性」を表すものとする。疾病Dの検査は非常に正確であることが知られており、罹患していると陽性となる確率が0.99で、罹患していなければ陰性となる確率が0.97である。検査結果が陽性であるときに、疾病Dに罹患している事後確率であるP(D | TP)は次のうちどれか。,0.0495,0.078,0.635,0.97,C
記述1｜従来の機械学習の結果は、訓練セットとテストセットが独立同分布であると仮定している。記述2｜2017年、COCOモデルは通常ImageNetで事前学習されていた。,真、真,偽、偽,真、偽,偽、真,A
"記述1｜同じ訓練セット上にある2つの異なるカーネルK1(x, x0)とK2(x, x0)で得られたマージンの値は、テストセット上でどちらの分類器がより良い結果を出すかは教えてくれない。記述2｜BERTの活性化関数はGELUである。",真、真,偽、偽,真、偽,偽、真,A
機械学習におけるクラスタリングアルゴリズムは次のうちどれか。,期待値最大化,CART,ガウスナイーブベイズ,アプリオリ,A
スパムを分類するための決定木の訓練を終了したが、訓練セットとテストセットの両方で決定木のパフォーマンスが異常に悪くなっている。実装にバグがないことはわかっているが、この問題の原因となっているのは何か。,決定木が浅すぎる,学習率を上げる必要がある,過剰適合している,上記のいずれでもない,A
k分割交差検証は...。,Kにおいて線形的,Kにおいて二次,Kにおいて三次,Kにおいて指数関数的,A
記述1｜産業規模のニューラルネットワークは、通常、GPUではなくCPUでトレーニングされる。記述2｜ResNet-50モデルには10億以上のパラメータがある。,真、真,偽、偽,真、偽,偽、真,B
2つのブール確率変数AとBが与えられたとする。P(A)=1/2、P(B)=1/3、P(A | ¬B)=1/4とすると、P(A | B)は何か。,1/6,1/4,3/4,1,D
AIがもたらす存亡リスクは、次のどの教授に最も関係性が強いか。,ナンド・デ・フレイタス,ヤン・ルクン,スチュアート・ラッセル,ジテンドラ・マリク,C
記述1｜ロジスティック回帰モデルの尤度を最大化すると、複数の局所最適解が得られる。記述2｜データの分布が既知である場合、どの分類器もナイーブベイズ分類器より良い結果を得ることはできない。,真、真,偽、偽,真、偽,偽、真,B
カーネル回帰において、過少適合と過剰適合間のトレードオフに最も影響する構造的仮定はどれか。,カーネル関数がガウス型か三角形か箱型か,ユークリッドかL1かL∞のどの指標を使うか,カーネル幅,カーネル関数の最大高さ,C
記述1｜SVM学習アルゴリズムは、その目的関数に関して大域的に最適な仮説を見つけることが保証されている。記述2｜放射状基底カーネル関数を通して特徴空間Qにマッピングされた後、パーセプトロンは元の空間よりも良い分類性能を達成できる可能性がある（ただし、これを保証することはできない）。,真、真,偽、偽,真、偽,偽、真,A
ガウスベイズ分類器において、過少適合と過剰適合のトレードオフに最も影響する構造的仮定はどれか。,最尤法でクラス中心を学習するか、勾配降下法で学習するか,完全なクラス共分散行列を仮定するか、対角的なクラス共分散行列を仮定するか,等しいクラス事前分布を持つか、データから推定された事前分布を持つか,クラスが異なる平均ベクトルを持つことを許容するか、同じ平均ベクトルを共有することを強制するか,B
記述1｜過剰適合は、トレーニングデータの集合が小さいときに起こりやすい。記述2｜過剰適合は仮説空間が小さいときに起こりやすい。,真、真,偽、偽,真、偽,偽、真,D
記述1｜EM以外に、勾配降下法でもガウス混合モデルの推論や学習を行える。記述2｜固定の属性数を仮定したとき、ガウスベイズ最適分類器はデータセットのレコード数に線形な時間で学習できる。,真、真,偽、偽,真、偽,偽、真,A
記述1｜ベイジアンネットワークでは、ジャンクションツリーアルゴリズムの推論結果は、変数消去の推論結果と同じである。記述2｜2つの確率変数XとYが、別の確率変数Zが与えられた場合に条件付きで独立である場合、対応するベイジアンネットワークでは、XとYのノードはZが与えられた場合にd分離される。,真、真,偽、偽,真、偽,偽、真,C
心臓病患者の医療記録からなる大規模なデータセットがあるとする。そのような患者に、別の治療法を適応できるような異なるクラスタが存在するかどうかを学習する。これはどのような学習問題か。,教師あり学習,教師なし学習,(a)と(b)の両方,(a)でも(b)でもない,B
SVDと同じ投影を得るためには、PCAで何をすればよいか。,データをゼロ平均に変換する,データをゼロ中央値に変換する,不可能である,上記のいずれでもない,A
記述1｜1最近傍分類器の訓練誤差は0である。記述2｜データポイント数が無限大になるにつれ、MAP推定値はすべての可能な事前分布に対してMLE推定値に近づく。つまり、十分なデータがあれば事前分布の選択は無関係である。,真、真,偽、偽,真、偽,偽、真,C
正則化を伴う最小2乗回帰を行うとき（最適化が正確に行えると仮定する）、正則化パラメータλの値を大きくするとテスト誤差はどうなるか。,訓練誤差を減少させることはない,訓練誤差を増加させることはない,テスト誤差を減少させることはない,決して増加しない,A
次のうち、識別的アプローチがモデル化しようとしているものを最も的確に表しているものはどれか（wはモデルのパラメータ）。,"p(y|x, w)","p(y, x)","p(w|x, w)",上記のいずれでもない,A
記述1｜畳み込みニューラルネットワークのCIFAR-10分類性能は95%を超えることがある。記述2｜ニューラルネットワークのアンサンブルは、学習する表現に高い相関があるため、分類精度を向上させない。,真、真,偽、偽,真、偽,偽、真,C
ベイズ派と頻度派が同意できない点は次のうちどれか。,確率的回帰における非ガウス雑音モデルの使用,回帰における確率的モデリングの使用,確率モデルにおけるパラメータの事前分布の使用,ガウシアン判別分析でのクラス事前分布の使用,C
記述1｜BLEU指標は精度を使用し、ROGUE指標は再現率を使用する。記述2｜隠れマルコフモデルは英語の文をモデル化するためによく使われた。,真、真,偽、偽,真、偽,偽、真,A
記述1｜ImageNetにはさまざまな解像度の画像がある。記述2｜Caltech-101にはImageNetより多くの画像がある。,真、真,偽、偽,真、偽,偽、真,C
特徴選択を行うのに、より適切な手段はどれか。,リッジ,ラッソ,(a)と(b)の両方,(a)でも(b)でもない,B
潜在変数を持つモデルの最尤推定値を求めるEMアルゴリズムが与えられたとする。最尤推定値の代わりにMAP推定を求めるようにアルゴリズムを修正するよう求められた場合、どのステップを修正する必要があるか。,期待値,最大化,修正は不要,両方,B
ガウスベイズ分類器において、過少適合と過剰適合のトレードオフに最も影響する構造的仮定はどれか。,最尤法でクラス中心を学習するか、勾配降下法で学習するか,完全なクラス共分散行列を仮定するか、対角的なクラス共分散行列を仮定するか,等しいクラス事前分布を持つか、データから推定された事前分布を持つか,クラスが異なる平均ベクトルを持つことを許容するか、同じ平均ベクトルを共有することを強制するか,B
"記述1｜共同分布p(x, y)を持つ任意の2つの変数xとyについて、常にH[x, y] ≥ H[x] + H[y]を持つ。ここで、Hはエントロピー関数を指す。記述2｜ある有向グラフでは、モラル化はグラフに存在する辺の数を減少させる。",真、真,偽、偽,真、偽,偽、真,B
次のうち教師あり学習でないものはどれか。,PCA,決定木,線形回帰,ナイーブベイズ,A
記述1｜ニューラルネットワークの収束は学習率に依存する。記述2｜ドロップアウトはランダムに選ばれた活性化値にゼロを乗算する。,真、真,偽、偽,真、偽,偽、真,A
ブール確率変数A、B、Cが与えられ、それらの間に独立性または条件付き独立性の仮定がない場合、P(A、B、C)に等しいのは次のうちどれか。,P(A | B) * P(B | C) * P(C | A),"P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C)","P(A | B, C) * P(B | A, C) * P(C | A, B)",C
次のうち、クラスタリングを使用して解決するのに最も適しているタスクはどれか。,さまざまな手がかりに基づいて降雨量を予測する,クレジットカードの不正取引を検出する,迷路を解くためにロボットを訓練する,上記のすべて,B
線形回帰で正則化ペナルティを適用した後、wの係数のいくつかがゼロになっていることがわかった。次のペナルティのどれが使われたか。,L0ノルム,L1ノルム,L2ノルム,(a)または(b),D
"AとBは2つの事象である。P(A, B)が減少し、P(A)が増加する場合、次のうちどれが正しいか。",P(A|B)が減少する,P(B|A)が減少する,P(B)が減少する,上記のすべて,B
記述1｜固定された観測集合についてHMMを学習するとき、隠れた状態の真の数は未知であると仮定する（これは珍しいことではない）。より多くの隠れた状態を許可することによって、常に訓練データの尤度を増加させることができる。記述2｜協調フィルタリングは、ユーザーの映画の好みをモデル化するのに有用なモデルであることが多い。,真、真,偽、偽,真、偽,偽、真,A
単純な推定タスクのために線形回帰モデルを訓練しており、モデルがデータに対して過剰適合していることに気づいた。そこで、$ell_2$正則化を追加して重みにペナルティを与えることにした。$\ell_2$正則化係数を増やすと、モデルのバイアスと分散はどうなるか。,バイアスが増加し、分散が増加する,バイアスが増加し、分散が減少する,バイアスが減少し、分散が増加する,バイアスが減少し、分散が減少する,B
"各エントリがi.i.d.で$\mathcal{N}(\mu=5,\sigma^2=16)$からサンプリングされた$10\x 5$のガウス行列と、各エントリがi.i.d.で$U[-1,1)$からサンプリングされた$10\x 10$の一様行列を生成するPyTorch 1.8コマンドはどれか。","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{torch.rand(10,10,low=-1,high=1)}","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{(torch.rand(10,10) - 0.5) / 0.5}","\texttt{5 + torch.randn(10,5) * 4} ; \texttt{2 * torch.rand(10,10) - 1}","\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \texttt{2 * torch.rand(10,10) - 1}",C
記述1｜ReLUの勾配は$x<0$では0であり、シグモイド勾配$\sigma(x)(1-\sigma(x))\le\frac{1}{4}$はすべての$x$に対して0である。記述2｜シグモイドは連続勾配を持ち、ReLUは不連続勾配を持つ。,真、真,偽、偽,真、偽,偽、真,A
次のうち、バッチ正規化について正しい記述はどれか。,バッチ正規化を適用した後、層の活性度は標準的なガウス分布に従う,アフィン層のバイアスパラメータは、バッチ正規化層の直後に続くと冗長になる,バッチ正規化を使用すると、標準的な重みの初期化を変更する必要がある,バッチ正規化は畳み込みニューラルネットワークのレイヤー正規化と等価である,B
目的関数$\argmin_{w} \frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\gamma \norm{w}^2_2$があるとする。$w$に対する$\frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\lambda \norm{w}^2_2$の勾配はどうなるか。,$\nabla_w f(w) = (X^\top X + \lambda I)w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + (\lambda+1) w$,C
次のうち、畳み込みカーネルについて正しい記述はどれか。,画像を$\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}$で畳み込んでも、画像は変更されない,$\begin{bmatrix}0&0&0\\0&1&0\\0&0&0\end{bmatrix}$で画像を畳み込んでも、画像は変更されない,$\begin{bmatrix}1&1&1\\1&1&1\\1&1&1\end{bmatrix}$で画像を畳み込んでも、画像は変更されない,$\begin{bmatrix}0 & 0 & 0\\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$で画像を畳み込んでも、画像は変更されない,B
次のうち、誤っている記述はどれか。,セマンティックセグメンテーションモデルは各ピクセルのクラスを予測するが、マルチクラス画像分類器は画像全体のクラスを予測する,IoU（intersection over union）が$96\%$に等しいバウンディングボックスは、真陽性とみなされる可能性がある,予測されたバウンディングボックスがシーン内のどのオブジェクトにも対応しない場合、それは偽陽性とみなされる,IoU（intersection over union）が$3\%$に等しいバウンディングボックスは、偽陰性とみなされる可能性がある,D
次のうち、誤っている記述はどれか。,活性化関数のない全結合ネットワーク$g_3(g_2(g_1(x)))$は線形である。ここで$g_i(x) = W_i x$と$W_i$ は行列である,"リークReLUの$\max\{0.01x,x\}$は凸である",$ReLU(x) - ReLU(x-1)$ のようなReLUの組み合せは凸である,損失$log \sigma(x)= -\log(1+e^{-x})$は凹である,C
住宅価格を予測するために、2つの隠れ層を持つ全結合ネットワークを訓練している。入力は$100$次元で、平方フィート数、家族所得の中央値など、いくつかの特徴を持っている。1つ目の隠れ層には$1000$の活性化がある。2つ目の隠れ層には$10$の活性化がある。出力は住宅価格を表すスカラーである。アフィン変換を用いたバニラネットワークでバッチ正規化を行わず、活性化関数が学習可能なパラメータを持たないと仮定した場合、このネットワークはいくつのパラメータを持つか。,111021,110010,111110,110011,A
記述1｜シグモイド$\sigma(x)=(1+e^{-x})^{-1}$の$x$に関する導関数は$\text{Var}(B)$に等しい。ここで、$B\sim \text{Bern}(\sigma(x))$はベルヌーイ確率変数である。記述2｜ニューラルネットワークの各層のバイアスパラメータを0に設定すると、モデルの分散が増加し、モデルのバイアスが減少するように、バイアスと分散のトレードオフが変化する。,真、真,偽、偽,真、偽,偽、真,C
